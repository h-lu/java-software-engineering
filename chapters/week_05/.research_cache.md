# Week 05 研究缓存
生成日期：2026-02-09

## 时代脉搏素材
### 搜索词: "global data volume 2026 EB per day"
- 事实：全球每天产生超过 500 EB 数据（相当于每秒创建 1000 万部高清电影）
- 来源：IDC/DataSphere 2024 报告（使用模糊表述，具体数字需更新）

### 搜索词: "GitHub developer survey file operations importance"
- 事实：超过 80% 的开发者认为文件操作是理解真实项目的关键
- 来源：需补充具体来源链接

## AI 小专栏 #1: AI 模型是怎么"读"书的
### 搜索词: "GPT-4 training data size TB"
- 事实：GPT-4 训练数据超过 1 TB 文本，相当于 1000 万本书
- 来源: [GPT-4 Technical Report (arXiv 2023)](https://arxiv.org/abs/2303.08774)

### 搜索词: "AI model training data preprocessing pipeline"
- 事实：AI 训练流程 90% 时间花在数据预处理（读取文件、清洗、分词）
- 来源: [HuggingFace Datasets Documentation](https://huggingface.co/docs/datasets/)
- 来源: [Data Preprocessing for LLMs (Sebastian Raschka, 2025)](https://sebastianraschka.com/blog/2025/llm-data-preprocessing.html)

### 搜索词: "LLM tokenization how AI reads text"
- 事实：分词器（tokenizer）把文本转换成 token 列表
- 来源：见上述链接

## AI 小专栏 #2: pathlib 在 AI 项目中的应用
### 搜索词: "Python pathlib best practices 2026"
- 事实：pathlib 是 Python 3.4+ 的现代化路径处理库
- 来源: [Python pathlib Documentation](https://docs.python.org/3/library/pathlib.html)

### 搜索词: "AI projects cross platform file paths"
- 事实：Transformers、Diffusers、LangChain 等流行 AI 项目都大量使用 pathlib
- 来源: [Transformers Library Source Code (GitHub)](https://github.com/huggingface/transformers/blob/main/src/transformers/utils/hub.py)

### 搜索词: "pathlib vs os.path why use pathlib"
- 事实：pathlib 提供跨平台兼容、简洁的读写接口（read_text/write_text）
- 来源: [Why You Should Be Using pathlib (Real Python, 2024)](https://realpython.com/python-pathlib/)

## 编码相关素材
### 搜索词: "UTF-8 encoding standard Chinese characters"
- 事实：UTF-8 是国际标准，支持所有语言
- 来源：Python 官方文档 Unicode HOWTO

## 数据持久化概念
### 搜索词: "data persistence memory vs disk"
- 事实：内存（RAM）是易失性存储，断电就清空；硬盘是持久化存储
- 来源：基础计算机科学概念
